{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:19.010645Z",
     "iopub.status.busy": "2025-03-13T16:00:19.010378Z",
     "iopub.status.idle": "2025-03-13T16:00:26.498288Z",
     "shell.execute_reply": "2025-03-13T16:00:26.497302Z",
     "shell.execute_reply.started": "2025-03-13T16:00:19.010622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:28.257124Z",
     "iopub.status.busy": "2025-03-13T16:00:28.256797Z",
     "iopub.status.idle": "2025-03-13T16:00:28.344602Z",
     "shell.execute_reply": "2025-03-13T16:00:28.343950Z",
     "shell.execute_reply.started": "2025-03-13T16:00:28.257104Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    Arabic  \\\n",
      "acm eng  اذا الواحد يستخدم الفلوس بحمكة يكدر يسوي كومة ...   \n",
      "    eng                                 عمرك رايح المكسيك؟   \n",
      "    eng                       فكرنا انه طبيعي لازم يتعاقب.   \n",
      "    eng              لازم تترك الامور تاخذ مجراها الطبيعي.   \n",
      "    eng                                    لا يريدون استخ.   \n",
      "\n",
      "                                                   English  \n",
      "acm eng                 If wisely used, money can do much.  \n",
      "    eng                      Have you ever been to Mexico?  \n",
      "    eng  We thought that it was natural that he should ...  \n",
      "    eng         You must let things take their own course.  \n",
      "    eng                         They don't want to use it.  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('tatoeba-dev.ara-eng.tsv', sep='\\t', encoding='utf-8', header=None, names=['Arabic', 'English'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:28.351407Z",
     "iopub.status.busy": "2025-03-13T16:00:28.351108Z",
     "iopub.status.idle": "2025-03-13T16:00:28.372338Z",
     "shell.execute_reply": "2025-03-13T16:00:28.371745Z",
     "shell.execute_reply.started": "2025-03-13T16:00:28.351371Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_cleaned = data.dropna()\n",
    "data = data.drop_duplicates()\n",
    "def remove_diacritics(text):\n",
    "    arabic_diacritics = re.compile(r'[\\u064B-\\u0652]') \n",
    "    text = re.sub(arabic_diacritics, '', text)  \n",
    "    text = text.replace(\"ى\", \"ي\").replace(\"ة\", \"ه\") \n",
    "    return text.strip()\n",
    "\n",
    "data['Arabic'] = data['Arabic'].apply(remove_diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:28.457124Z",
     "iopub.status.busy": "2025-03-13T16:00:28.456887Z",
     "iopub.status.idle": "2025-03-13T16:00:28.474194Z",
     "shell.execute_reply": "2025-03-13T16:00:28.473522Z",
     "shell.execute_reply.started": "2025-03-13T16:00:28.457094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data['English'] = data['English'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:28.475192Z",
     "iopub.status.busy": "2025-03-13T16:00:28.474926Z",
     "iopub.status.idle": "2025-03-13T16:00:28.536720Z",
     "shell.execute_reply": "2025-03-13T16:00:28.536123Z",
     "shell.execute_reply.started": "2025-03-13T16:00:28.475172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(\"cleaned_dataset.tsv\", sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:28.537758Z",
     "iopub.status.busy": "2025-03-13T16:00:28.537544Z",
     "iopub.status.idle": "2025-03-13T16:00:28.562179Z",
     "shell.execute_reply": "2025-03-13T16:00:28.561601Z",
     "shell.execute_reply.started": "2025-03-13T16:00:28.537728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def write_sentences_to_file(sentences, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence.strip() + '\\n')\n",
    "\n",
    "write_sentences_to_file(data_cleaned['Arabic'], 'arabic_sentences.txt')\n",
    "write_sentences_to_file(data_cleaned['English'], 'english_sentences.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:28.563178Z",
     "iopub.status.busy": "2025-03-13T16:00:28.562923Z",
     "iopub.status.idle": "2025-03-13T16:00:29.613284Z",
     "shell.execute_reply": "2025-03-13T16:00:29.612420Z",
     "shell.execute_reply.started": "2025-03-13T16:00:28.563146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='arabic.txt',\n",
    "    model_prefix='spm_arabic',\n",
    "    vocab_size=15475,\n",
    "    model_type='unigram', \n",
    "    character_coverage=1.0,\n",
    "    pad_id=0, unk_id=1, bos_id=2, eos_id=3  \n",
    ")\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='english.txt',\n",
    "    model_prefix='spm_english',\n",
    "    vocab_size=6897,\n",
    "    model_type='unigram',\n",
    "    character_coverage=1.0,\n",
    "    pad_id=0, unk_id=1, bos_id=2, eos_id=3\n",
    ")\n",
    "\n",
    "arabic_sp = spm.SentencePieceProcessor(model_file='arabic.model')\n",
    "english_sp = spm.SentencePieceProcessor(model_file='english.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:29.814987Z",
     "iopub.status.busy": "2025-03-13T16:00:29.814748Z",
     "iopub.status.idle": "2025-03-13T16:00:29.819664Z",
     "shell.execute_reply": "2025-03-13T16:00:29.818847Z",
     "shell.execute_reply.started": "2025-03-13T16:00:29.814968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_sequence(sp_processor, sentence, max_len, bos_id, eos_id, pad_id):\n",
    "    tokens = sp_processor.encode(sentence, out_type=int)\n",
    "    tokens = [bos_id] + tokens + [eos_id]  \n",
    "    return pad_sequence(tokens, max_len, pad_id)\n",
    "\n",
    "def pad_sequence(tokens, max_len, pad_id):\n",
    "    \"\"\"Pad or truncate sequence to max_len.\"\"\"\n",
    "    return tokens[:max_len] + [pad_id] * max(0, max_len - len(tokens)) if len(tokens) < max_len else tokens[:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:30.814711Z",
     "iopub.status.busy": "2025-03-13T16:00:30.814460Z",
     "iopub.status.idle": "2025-03-13T16:00:31.295832Z",
     "shell.execute_reply": "2025-03-13T16:00:31.294987Z",
     "shell.execute_reply.started": "2025-03-13T16:00:30.814691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_len = 100\n",
    "arabic_pad_id = 0  \n",
    "english_sos_id = 2  \n",
    "english_eos_id = 3  \n",
    "english_pad_id = 0  \n",
    "\n",
    "arabic_sequences = [pad_sequence(arabic_sp.encode(sentence, out_type=int), max_len, arabic_pad_id) \n",
    "                   for sentence in data_cleaned['Arabic']]\n",
    "english_sequences = [preprocess_sequence(english_sp, sentence, max_len, english_sos_id, english_eos_id, english_pad_id) \n",
    "                    for sentence in data_cleaned['English']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:32:44.631041Z",
     "iopub.status.busy": "2025-03-13T15:32:44.630734Z",
     "iopub.status.idle": "2025-03-13T15:32:45.667821Z",
     "shell.execute_reply": "2025-03-13T15:32:45.666824Z",
     "shell.execute_reply.started": "2025-03-13T15:32:44.631008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data, temp_data = train_test_split(data_cleaned, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_arabic_sequences = [pad_sequence(arabic_sp.encode(sentence, out_type=int), max_len, 0) \n",
    "                        for sentence in train_data['Arabic']]\n",
    "train_english_sequences = [preprocess_sequence(english_sp, sentence, max_len, 2, 3, 0) \n",
    "                          for sentence in train_data['English']]\n",
    "val_arabic_sequences = [pad_sequence(arabic_sp.encode(sentence, out_type=int), max_len, 0) \n",
    "                       for sentence in val_data['Arabic']]\n",
    "val_english_sequences = [preprocess_sequence(english_sp, sentence, max_len, 2, 3, 0) \n",
    "                        for sentence in val_data['English']]\n",
    "test_arabic_sequences = [pad_sequence(arabic_sp.encode(sentence, out_type=int), max_len, 0) \n",
    "                        for sentence in test_data['Arabic']]\n",
    "test_english_sequences = [preprocess_sequence(english_sp, sentence, max_len, 2, 3, 0) \n",
    "                         for sentence in test_data['English']]\n",
    "\n",
    "train_dataset = TranslationDataset(torch.tensor(train_arabic_sequences), torch.tensor(train_english_sequences))\n",
    "val_dataset = TranslationDataset(torch.tensor(val_arabic_sequences), torch.tensor(val_english_sequences))\n",
    "test_dataset = TranslationDataset(torch.tensor(test_arabic_sequences), torch.tensor(test_english_sequences))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T15:32:55.416697Z",
     "iopub.status.busy": "2025-03-13T15:32:55.416371Z",
     "iopub.status.idle": "2025-03-13T15:32:56.409349Z",
     "shell.execute_reply": "2025-03-13T15:32:56.408320Z",
     "shell.execute_reply.started": "2025-03-13T15:32:55.416650Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
    "\n",
    "arabic_tokenizer = Tokenizer(models.BPE())\n",
    "arabic_pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "arabic_tokenizer.pre_tokenizer = arabic_pre_tokenizer\n",
    "arabic_trainer = trainers.BpeTrainer(vocab_size=20000, special_tokens=[\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"])\n",
    "\n",
    "arabic_tokenizer.train(files=[\"arabic_sentences.txt\"], trainer=arabic_trainer)\n",
    "arabic_tokenizer.save(\"arabic_bpe_tokenizer.json\")\n",
    "\n",
    "english_tokenizer = Tokenizer(models.BPE())\n",
    "english_pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "english_tokenizer.pre_tokenizer = english_pre_tokenizer\n",
    "english_trainer = trainers.BpeTrainer(vocab_size=10000, special_tokens=[\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"])\n",
    "\n",
    "english_tokenizer.train(files=[\"english_sentences.txt\"], trainer=english_trainer)\n",
    "english_tokenizer.save(\"english_bpe_tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:34.804970Z",
     "iopub.status.busy": "2025-03-13T16:00:34.804656Z",
     "iopub.status.idle": "2025-03-13T16:00:38.220377Z",
     "shell.execute_reply": "2025-03-13T16:00:38.219634Z",
     "shell.execute_reply.started": "2025-03-13T16:00:34.804943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "arabic_tensors = torch.tensor(arabic_sequences, dtype=torch.long)\n",
    "english_tensors = torch.tensor(english_sequences, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:38.221752Z",
     "iopub.status.busy": "2025-03-13T16:00:38.221447Z",
     "iopub.status.idle": "2025-03-13T16:00:38.227802Z",
     "shell.execute_reply": "2025-03-13T16:00:38.226852Z",
     "shell.execute_reply.started": "2025-03-13T16:00:38.221728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_tensors, tgt_tensors):\n",
    "        self.src_tensors = src_tensors\n",
    "        self.tgt_tensors = tgt_tensors\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.src_tensors)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_tensors[idx], self.tgt_tensors[idx]\n",
    "\n",
    "train_dataset = TranslationDataset(arabic_tensors, english_tensors)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:40.963811Z",
     "iopub.status.busy": "2025-03-13T16:00:40.963519Z",
     "iopub.status.idle": "2025-03-13T16:00:40.968370Z",
     "shell.execute_reply": "2025-03-13T16:00:40.967597Z",
     "shell.execute_reply.started": "2025-03-13T16:00:40.963778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:41.525472Z",
     "iopub.status.busy": "2025-03-13T16:00:41.525120Z",
     "iopub.status.idle": "2025-03-13T16:00:41.533571Z",
     "shell.execute_reply": "2025-03-13T16:00:41.532752Z",
     "shell.execute_reply.started": "2025-03-13T16:00:41.525442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model \n",
    "        self.num_heads = num_heads \n",
    "        self.d_k = d_model // num_heads \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model) \n",
    "        self.W_v = nn.Linear(d_model, d_model) \n",
    "        self.W_o = nn.Linear(d_model, d_model) \n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:41.812150Z",
     "iopub.status.busy": "2025-03-13T16:00:41.811902Z",
     "iopub.status.idle": "2025-03-13T16:00:41.816478Z",
     "shell.execute_reply": "2025-03-13T16:00:41.815656Z",
     "shell.execute_reply.started": "2025-03-13T16:00:41.812129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:42.129866Z",
     "iopub.status.busy": "2025-03-13T16:00:42.129651Z",
     "iopub.status.idle": "2025-03-13T16:00:42.134779Z",
     "shell.execute_reply": "2025-03-13T16:00:42.134057Z",
     "shell.execute_reply.started": "2025-03-13T16:00:42.129847Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:42.393169Z",
     "iopub.status.busy": "2025-03-13T16:00:42.392962Z",
     "iopub.status.idle": "2025-03-13T16:00:42.397922Z",
     "shell.execute_reply": "2025-03-13T16:00:42.397018Z",
     "shell.execute_reply.started": "2025-03-13T16:00:42.393150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:44.477316Z",
     "iopub.status.busy": "2025-03-13T16:00:44.477059Z",
     "iopub.status.idle": "2025-03-13T16:00:44.482646Z",
     "shell.execute_reply": "2025-03-13T16:00:44.481800Z",
     "shell.execute_reply.started": "2025-03-13T16:00:44.477296Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:44.740903Z",
     "iopub.status.busy": "2025-03-13T16:00:44.740705Z",
     "iopub.status.idle": "2025-03-13T16:00:44.747692Z",
     "shell.execute_reply": "2025-03-13T16:00:44.747035Z",
     "shell.execute_reply.started": "2025-03-13T16:00:44.740886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2).to(src.device)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3).to(tgt.device)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=tgt.device), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:47.403149Z",
     "iopub.status.busy": "2025-03-13T16:00:47.402815Z",
     "iopub.status.idle": "2025-03-13T16:00:48.345285Z",
     "shell.execute_reply": "2025-03-13T16:00:48.344615Z",
     "shell.execute_reply.started": "2025-03-13T16:00:47.403118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "src_vocab_size = 15000\n",
    "tgt_vocab_size = 6800  \n",
    "d_model = 512         \n",
    "num_heads = 8         \n",
    "num_layers = 6         \n",
    "d_ff = 2048           \n",
    "max_seq_length = 100    \n",
    "dropout = 0.3         \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:00:49.314060Z",
     "iopub.status.busy": "2025-03-13T16:00:49.313777Z",
     "iopub.status.idle": "2025-03-13T16:00:51.124232Z",
     "shell.execute_reply": "2025-03-13T16:00:51.123279Z",
     "shell.execute_reply.started": "2025-03-13T16:00:49.314028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=english_pad_id)  # Use english_pad_id (0)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    for src, tgt in dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        tgt_input = tgt[:, :-1].to(device)\n",
    "        tgt_output = tgt[:, 1:].to(device)\n",
    "        output = model(src, tgt_input)\n",
    "        loss = criterion(output.reshape(-1, tgt_vocab_size), tgt_output.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        predictions = output.argmax(dim=-1)\n",
    "        mask = (tgt_output != english_pad_id)\n",
    "        correct = (predictions == tgt_output) & mask\n",
    "        total_correct += correct.sum().item()\n",
    "        total_tokens += mask.sum().item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_tokens if total_tokens > 0 else 0\n",
    "    return avg_loss, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T16:01:07.711372Z",
     "iopub.status.busy": "2025-03-13T16:01:07.710813Z",
     "iopub.status.idle": "2025-03-13T17:38:26.109673Z",
     "shell.execute_reply": "2025-03-13T17:38:26.108729Z",
     "shell.execute_reply.started": "2025-03-13T16:01:07.711329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 43\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss, accuracy = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    scheduler.step(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "model_path = \"transformer_translation.pth\"\n",
    "torch.save(model.module if isinstance(model, nn.DataParallel) else model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T17:39:07.224833Z",
     "iopub.status.busy": "2025-03-13T17:39:07.224548Z",
     "iopub.status.idle": "2025-03-13T17:39:07.588072Z",
     "shell.execute_reply": "2025-03-13T17:39:07.587148Z",
     "shell.execute_reply.started": "2025-03-13T17:39:07.224810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def translate_sentence(model, arabic_sentence, arabic_sp, english_sp, max_len, device):\n",
    "    arabic_sentence = remove_diacritics(arabic_sentence)\n",
    "    src = pad_sequence(arabic_sp.encode(arabic_sentence, out_type=int), max_len, 0)\n",
    "    src = torch.tensor([src], dtype=torch.long).to(device)\n",
    "    \n",
    "    tgt = torch.tensor([[2]], dtype=torch.long).to(device)  # BOS = 2\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - 1):\n",
    "            output = model(src, tgt)\n",
    "            next_token = output[:, -1, :].argmax(dim=-1).item()\n",
    "            tgt = torch.cat([tgt, torch.tensor([[next_token]], dtype=torch.long).to(device)], dim=1)\n",
    "            if next_token == 3:  # EOS = 3\n",
    "                break\n",
    "    translated_ids = tgt[0].cpu().tolist()\n",
    "    if translated_ids[0] == 2:  \n",
    "        translated_ids = translated_ids[1:]\n",
    "    if translated_ids[-1] == 3:  \n",
    "        translated_ids = translated_ids[:-1]\n",
    "    \n",
    "    translated_sentence = english_sp.decode(translated_ids)\n",
    "    return translated_sentence\n",
    "arabic_test_sentence = \"أنا لا أحتاجك بعد الأن\"\n",
    "translated = translate_sentence(model, arabic_test_sentence, arabic_sp, english_sp, max_len=100, device=device)\n",
    "print(f\"Arabic: {arabic_test_sentence}\")\n",
    "print(f\"English: {translated}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11228134,
     "datasetId": 6748615,
     "sourceId": 10863294,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11249929,
     "datasetId": 6762469,
     "sourceId": 10882931,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11260216,
     "datasetId": 6768897,
     "sourceId": 10892207,
     "sourceType": "datasetVersion"
    },
    {
     "databundleVersionId": 11255660,
     "datasetId": 6765821,
     "sourceId": 10888111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
